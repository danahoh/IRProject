{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "body_stemming.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziHij-yBqvbO"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from operator import add\n",
        "import builtins\n",
        "from google.cloud import storage\n",
        "from collections import defaultdict\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ],
      "metadata": {
        "id": "Rr3Kbv0Xqw4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "xLEWrRapq3Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m nltk.downloader punkt"
      ],
      "metadata": {
        "id": "zE0TtcuVq5Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def open_gcp(file_name,dir_name):\n",
        "    client = storage.Client(file_name)\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.get_blob(f'postings_gcp{dir_name}/' + file_name)\n",
        "    return blob.open('rb')\n",
        "\n",
        "def read_pickle(file_name,dir_name):\n",
        "    stream = open_gcp(file_name+\".pickle\",dir_name)\n",
        "    pick = pickle.load(stream)\n",
        "    stream.close()\n",
        "    return pick\n",
        "\n",
        "def read_pkl(file_name,dir_name):\n",
        "    stream = open_gcp(file_name+\".pkl\",dir_name)\n",
        "    pick = pickle.load(stream)\n",
        "    stream.close()\n",
        "    return pick"
      ],
      "metadata": {
        "id": "GxutZ2vtq7og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_pickle(object_to_write,object_name ,bucket_name,dir_name):\n",
        "    with open(f\"{object_name}.pickle\", \"wb\") as f:\n",
        "        pickle.dump(object_to_write, f)\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob_posting_locs = bucket.blob(f\"postings_gcp{dir_name}/{object_name}.pickle\")\n",
        "    blob_posting_locs.upload_from_filename(f\"{object_name}.pickle\")\n",
        "\n",
        "def write_pkl(object_to_write,object_name ,bucket_name,dir_name):\n",
        "    with open(f\"{object_name}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(object_to_write, f)\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob_posting_locs = bucket.blob(f\"postings_gcp{dir_name}/{object_name}.pkl\")\n",
        "    blob_posting_locs.upload_from_filename(f\"{object_name}.pkl\")\n",
        "\n"
      ],
      "metadata": {
        "id": "32ge7WHSq92G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from graphframes import *"
      ],
      "metadata": {
        "id": "0BjOkXwbrAW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /usr/lib/spark/jars/graph*"
      ],
      "metadata": {
        "id": "aQg3uC0-rC7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = '318457645' \n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)"
      ],
      "metadata": {
        "id": "qAKq1vIarEwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ],
      "metadata": {
        "id": "G1RqoBGcrHdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inverted_index_gcp import *"
      ],
      "metadata": {
        "id": "Vw9-LV5grJju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_path = \"gs://wikidata_preprocessed/*\"\n"
      ],
      "metadata": {
        "id": "uvJquJz7rMOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = spark.read.parquet(full_path)\n",
        "id_text = df_data.select('id','text')\n",
        "id_text = id_text.rdd\n",
        "inverted_text = InvertedIndex()\n"
      ],
      "metadata": {
        "id": "mkwg9PxerOWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()"
      ],
      "metadata": {
        "id": "N-j8EMZirYwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
        "                    \"many\", \"however\", \"would\"]\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "\n",
        "def word_count(id, text):\n",
        "\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    tokens = [x for x in tokens if x not in all_stopwords]\n",
        "    tokens = [porter.stem(x) for x in tokens]\n",
        "\n",
        "    tuples = []\n",
        "    tf_dict = Counter(tokens)\n",
        "    res = []\n",
        "    [res.append(x) for x in tokens if x not in res]\n",
        "    for t in res:\n",
        "        tuples.append((t,(id,tf_dict[t])))\n",
        "    return tuples"
      ],
      "metadata": {
        "id": "AgzK3Oyhra9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_doc_len(doc_id, text):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    tokens = [x for x in tokens if x not in all_stopwords]\n",
        "    tokens = [porter.stem(x) for x in tokens]\n",
        "    return (doc_id,len(tokens))"
      ],
      "metadata": {
        "id": "tr541PCmrc8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_for_term_id(doc_id, text):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    tokens = [x for x in tokens if x not in all_stopwords]\n",
        "    tokens = [porter.stem(x) for x in tokens]\n",
        "    tuples = []\n",
        "    tf_dict = Counter(tokens)\n",
        "    res = []\n",
        "    [res.append(x) for x in tokens if x not in res]\n",
        "    for t in res:\n",
        "        tuples.append((t,tf_dict[t]))\n",
        "    return tuples"
      ],
      "metadata": {
        "id": "p-xbo9WnriHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts_text = id_text.flatMap(lambda x: word_count(x[0], x[1]))\n"
      ],
      "metadata": {
        "id": "ZCyihmysrkWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_word_counts(unsorted_pl):\n",
        "    return sorted(unsorted_pl)"
      ],
      "metadata": {
        "id": "FpQFxXBIrm0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "postings_text = word_counts_text.groupByKey().mapValues(reduce_word_counts)\n",
        "postings_filtered_text = postings_text.filter(lambda x: len(x[1])>50)\n"
      ],
      "metadata": {
        "id": "5Sipq1SWro-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_terms_text = postings_filtered_text.flatMapValues(lambda x : x).map(lambda x: (x[0],x[1][1])).reduceByKey(add)\n",
        "inverted_text.term_total = total_terms_text.collectAsMap()"
      ],
      "metadata": {
        "id": "VSj2jFxJrs5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_df(postings):\n",
        "    rdd2 = postings.map(lambda x: (x[0], len(x[1]))) \n",
        "    return rdd2"
      ],
      "metadata": {
        "id": "2JP1XPwJr11q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2df_text = calculate_df(postings_filtered_text)\n",
        "\n",
        "w2df_text_dict = w2df_text.collectAsMap()"
      ],
      "metadata": {
        "id": "lcRBzx-dr60P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "    return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def partition_postings_and_write(postings,dir_name):\n",
        "    rdd2 = postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey()\n",
        "    rdd3 = rdd2.map(lambda x: InvertedIndex.write_a_posting_list(x,bucket_name,dir_name))\n",
        "    return rdd3"
      ],
      "metadata": {
        "id": "tff-T68sr-FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "postings_locs_text = partition_postings_and_write(postings_filtered_text,'text').collect()\n"
      ],
      "metadata": {
        "id": "XYZhOmdRsApq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "super_posting_locs_text = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcptext'):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            super_posting_locs_text[k].extend(v)"
      ],
      "metadata": {
        "id": "1d3P7safsCpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverted_text.posting_locs = super_posting_locs_text\n",
        "inverted_text.df = w2df_text_dict"
      ],
      "metadata": {
        "id": "T2C5Lck7sE4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tfidf_doc(tf_term_doc, index):\n",
        "    DL = index.DL\n",
        "    N = len(DL)\n",
        "    df_dict = index.df\n",
        "    doc_term_tfidf = tf_term_doc.map(lambda x : (x[0] , math.sqrt(builtins.sum([math.pow((tf/DL[x[0]]) * (math.log(N/df_dict[term],10)),2) for term, tf in x[1] if term in df_dict and x[0] in index.DL]))))\n",
        "    return doc_term_tfidf"
      ],
      "metadata": {
        "id": "pKxCQTjosHHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_term_doc_text = id_text.map(lambda x : (x[0], tf_for_term_id(x[0], x[1])))\n",
        "tfidf_text = calculate_tfidf_doc(tf_term_doc_text, inverted_text)\n",
        "tfidf_dict_text = tfidf_text.collectAsMap()"
      ],
      "metadata": {
        "id": "cbYgdnkGsOgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_pickle(inverted_text.DL, \"text_DL\",bucket_name,\"text\" )"
      ],
      "metadata": {
        "id": "QHzqKjimsQ6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_pickle(inverted_text.term_total, \"text_term_total\",bucket_name,\"text\")"
      ],
      "metadata": {
        "id": "2NQoEaUqsUW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_pickle(inverted_text.vec_len_doc, \"vec_len_total\",bucket_name,\"text\")"
      ],
      "metadata": {
        "id": "CmCcSv3KsWil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverted_text.write_index('.', 'index_text')\n",
        "index_src = \"index_text.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/postings_gcptext/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ],
      "metadata": {
        "id": "hAZ9JZLSsZ0W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}