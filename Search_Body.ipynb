{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Search Body.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-HgJ4vssyUS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from operator import add\n",
        "import builtins\n",
        "from google.cloud import storage\n",
        "from collections import defaultdict\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ],
      "metadata": {
        "id": "xoBlsYFMszii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from graphframes import *"
      ],
      "metadata": {
        "id": "Fq6HqRz8s2Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def open_gcp(file_name,dir_name):\n",
        "    client = storage.Client(file_name)\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.get_blob(f'postings_gcp{dir_name}/' + file_name)\n",
        "    return blob.open('rb')\n",
        "\n",
        "def read_pickle(file_name,dir_name):\n",
        "    stream = open_gcp(file_name+\".pickle\",dir_name)\n",
        "    pick = pickle.load(stream)\n",
        "    stream.close()\n",
        "    return pick\n",
        "\n",
        "def read_pkl(file_name,dir_name):\n",
        "    stream = open_gcp(file_name+\".pkl\",dir_name)\n",
        "    pick = pickle.load(stream)\n",
        "    stream.close()\n",
        "    return pick"
      ],
      "metadata": {
        "id": "hUia2X_cs4Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ],
      "metadata": {
        "id": "E-TD1N5_s6df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = '318457645' \n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)"
      ],
      "metadata": {
        "id": "sNv3jYMBs8-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inverted_index_gcp import *"
      ],
      "metadata": {
        "id": "-Sompm9qs_fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverted_text = read_pkl('index','')"
      ],
      "metadata": {
        "id": "zzN4reXLtCEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverted_title = read_pkl('index_title','title')"
      ],
      "metadata": {
        "id": "g6j44L7WtD9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DL_dict = read_pickle('text_DL','')\n",
        "inverted_text.DL = DL_dict"
      ],
      "metadata": {
        "id": "T_zOM6zNtH19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_total_dict = read_pickle('text_term_total','')\n",
        "inverted_text.term_total = term_total_dict"
      ],
      "metadata": {
        "id": "NU6S0xautIux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_len_dict = read_pickle('vec_len_total', '')\n",
        "inverted_text.vec_len_doc = vec_len_dict"
      ],
      "metadata": {
        "id": "6F1Ug6SbtLI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TUPLE_SIZE = 6       \n",
        "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
        "from contextlib import closing\n",
        "\n",
        "def read_posting_list(inverted, w, base_dir):\n",
        "    with closing(MultiFileReader()) as reader:\n",
        "        locs = inverted.posting_locs[w]\n",
        "        b = reader.read(locs, inverted.df[w] * TUPLE_SIZE, base_dir)\n",
        "        posting_list = []\n",
        "        for i in range(inverted.df[w]):\n",
        "            doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
        "            tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
        "            posting_list.append((doc_id, tf))\n",
        "        return posting_list"
      ],
      "metadata": {
        "id": "TB144qR2tNL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_query_tfidf_vector(query_to_search,index):\n",
        "    \n",
        "    dict_term_tfidf = {}\n",
        "    counter = Counter(query_to_search)\n",
        "    for term in query_to_search:\n",
        "        if term in index.term_total.keys(): #avoid terms that do not appear in the index.               \n",
        "            tf = counter[term]/len(query_to_search) # term frequency divded by the length of the query\n",
        "            df = index.df[term]            \n",
        "            idf = math.log((len(index.DL))/(df),10) #smoothing\n",
        "            dict_term_tfidf[term] = tf*idf\n",
        "    return dict_term_tfidf"
      ],
      "metadata": {
        "id": "y2fuibIrtQcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def get_candidate_documents_and_scores(query_to_search,index,base_dir):\n",
        "\n",
        "    candidates = {}\n",
        "    N = len(index.DL)\n",
        "    for term in np.unique(query_to_search):  \n",
        "        if term in index.df.keys():\n",
        "            list_of_doc = read_posting_list(index, term, base_dir)\n",
        "            normlized_tfidf = [(doc_id,(freq/index.DL[doc_id])*math.log(N/index.df[term],10)) for doc_id, freq in list_of_doc]           \n",
        "            for doc_id, tfidf in normlized_tfidf:\n",
        "                candidates[(doc_id,term)] = candidates.get((doc_id,term), 0) + tfidf               \n",
        "    return candidates"
      ],
      "metadata": {
        "id": "BHt3CaCntUXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cosine_dict(query_to_search,index,base_dir):\n",
        "    \n",
        "    \n",
        "    cosine_dict = {}\n",
        "    candidates_scores = get_candidate_documents_and_scores(query_to_search,index,base_dir) #We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.\n",
        "    dict_query = generate_query_tfidf_vector(query_to_search,index)\n",
        "    for key in candidates_scores:\n",
        "        doc_id, term = key\n",
        "        cosine_dict[doc_id] = cosine_similarity(candidates_scores[key],dict_query,index,doc_id) \n",
        "    return cosine_dict"
      ],
      "metadata": {
        "id": "xhYY1GbMtWle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(tfidf_doc,dict_query,index,doc_id):\n",
        "    \n",
        "    #for one query and d\n",
        "    cos_sim_mone = 0.0\n",
        "    query_size_vec = 0.0\n",
        "    for term in dict_query.keys():\n",
        "        cos_sim_mone += dict_query[term] * tfidf_doc\n",
        "        query_size_vec += math.pow(dict_query[term],2)\n",
        "    cos_sim_total = cos_sim_mone / (index.vec_len_doc[doc_id] * (math.sqrt(query_size_vec)))\n",
        "    return cos_sim_total"
      ],
      "metadata": {
        "id": "ybElQIdttYxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n(query,index, base_dir,N=3):\n",
        "    \n",
        "#     vec_query = generate_query_tfidf_vector(query,index)\n",
        "    sim_dict = generate_cosine_dict(query,index, base_dir)\n",
        "    lst = [(doc_id , builtins.round(score,5)) for doc_id, score in sim_dict.items()]\n",
        "    return sorted(lst, key = lambda x: x[1], reverse=True)[:N]\n",
        "    "
      ],
      "metadata": {
        "id": "KqeFwnrFtbpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_q = {}\n",
        "dict_q[1] = ['python']\n",
        "tfidf_queries_score_train = get_top_n(['python'], inverted_text,'' , 100)\n",
        "print(tfidf_queries_score_train)\n",
        "\n",
        "# lst = [57069491, 65967176, 42163310, 878659, 27306717, 41677925, 1074657, 44240443, 17296107, 60952488, 43603241, 22114132, 46208997, 36450985, 41974555, 56289672, 60616450, 59502488, 33038861, 61699239, 61651800, 39368416, 29129051, 55935213, 54537218, 62372638, 60774345, 63090183, 37497391, 51430647, 67229718, 9110929, 61329320, 44254295, 41974496, 200563, 58481694, 48530084, 56289572, 22144990, 612052, 59162931, 55511148, 55511147, 61073786, 59892, 36484005, 36484254, 66423851, 62482816, 1275470, 5676692, 7927053, 60754840, 26999426, 60744481, 56289553, 60463979, 701741, 60283633, 1129847, 36439749, 4451883, 55511155, 22144721, 45359871, 723126, 43655965, 57275457, 12673434, 43867095, 26763420, 39293265, 15003874, 41668588, 61592102, 67063919, 11891433, 64057670, 61513780, 39345917, 67063906, 1221476, 41008758, 60587000, 7729, 2152196, 5027882, 509738, 403585, 26866372, 1339248, 3473503, 4148655]\n",
        "lst = [23862, 23329, 53672527, 21356332, 4920126, 5250192, 819149, 46448252, 83036, 88595, 18942, 696712, 2032271, 1984246, 5204237, 645111, 18384111, 3673376, 25061839, 271890, 226402, 2380213, 1179348, 15586616, 50278739, 19701, 3596573, 4225907, 19160, 1235986, 6908561, 3594951, 18805500, 5087621, 25049240, 2432299, 381782, 9603954, 390263, 317752, 38007831, 2564605, 13370873, 2403126, 17402165, 23678545, 7837468, 23954341, 11505904, 196698, 34292335, 52042, 2247376, 15858, 11322015, 13062829, 38833779, 7800160, 24193668, 440018, 54351136, 28887886, 19620, 23045823, 43003632, 746577, 1211612, 8305253, 14985517, 30796675, 51800, 964717, 6146589, 13024, 11583987, 57294217, 27471338, 5479462]\n",
        "lst_docs_returned = [x[0] for x in tfidf_queries_score_train]\n",
        "\n",
        "lst_equal = [x for x in lst_docs_returned if x in lst]\n",
        "print(len(lst))\n",
        "print(len(lst_equal))\n",
        "\n",
        "lst_indecies = [lst_docs_returned.index(x) for x in lst if x in lst_docs_returned]\n",
        "print(lst_indecies)"
      ],
      "metadata": {
        "id": "z6BmIuTitduq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}